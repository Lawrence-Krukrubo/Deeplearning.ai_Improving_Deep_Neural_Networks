# Improving_Deep_Neural_Networks

This project is the second course in the deeplearning.ai specialisation. It involves specific and deliberate steps in improving Deep Neural Networks, such as:-
* **Hyper-parameter tuning**
* **Regularization** 
* **Optimization**

#### The project is broken down into weekly tasks...

## Week 1:

### Key Concepts for this week include:
* Recall that different types of initializations lead to different results
* Recognize the importance of initialization in complex neural networks.
* Recognize the difference between train/dev/test sets
* Diagnose the bias and variance issues in your model
* Learn when and how to use regularization methods such as dropout or L2 regularization.
* Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them
* Use gradient checking to verify the correctness of your backpropagation implementation

### Tasks for week 1:
**Setting up your Machine Learning Application:**
This includes activities such as:-
* Understanding the importance and split of Train\Dev\Test sets.
* Bias \ Variance Trade-off.
* Basic recipe for machine learning.

**Regularizing your neural network:**
Activities here include:-
* L2 Regularization
* The Frobenius Norm Regularisation
* Dropout Regularisation.
* Understanding the Inverse-Dropout Technique.
* Other Regularisation methods.

**Setting up your optimization problem**
Activities here include:
* Normalisation of inputs - Z-score norm.
* Vanishing / Exploding Gradients.
* Weight initialisation for Deep Networks.
* Numerical approximation of gradients.
* Gradient checking.

**Notebook Week 1:**
The first notebook for this week is titled **initialization**. It contains experiments that show the ideal weight initialization techniques to use. 

## License:
This project and all its materials and docs abide under the MIT License.
